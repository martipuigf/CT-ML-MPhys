# -*- coding: utf-8 -*-
"""U_Net_4_superresolution_in_EM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bTbbAyUQHMTCQ7nR3nqjWMnRAXa3K5-j

Training School # 4 of NEUBIAS COST Action
February 29th-March 3rd, 2020, Bordeaux

# Deep Learning example: U-Net for super-resolution

---
## Introduction
This is a notebook that shows how to design and train a U-Net-like network for super-resolution on Electron Miscroscopy (EM) images. The aim is to train the network using low resolution versions of the images as input, and the high resolution versions as output.

<figure>
<center>
<img src="https://drive.google.com/uc?id=1KUCwas63FD6AfiKOetiGjLRR6oldNkKC" width="450">
</figure>



## Data
The image data used in the notebook was produced by [Lichtman Lab at Harvard University](https://lichtmanlab.fas.harvard.edu/) (Daniel R. Berger, Richard Schalek, Narayanan "Bobby" Kasthuri, Juan-Carlos Tapia, Kenneth Hayworth, Jeff W. Lichtman). Their corresponding biological findings were published in [Cell (2015)](https://www.ncbi.nlm.nih.gov/pubmed/26232230).
The training and test data sets are both 3D stacks of 100 sections from a serial section Scanning Electron Microscopy (ssSEM) data set of mouse cortex. The microcube measures 6 x 6 x 3 microns approx., with a resolution of 6 x 6 x 30 nm/voxel. For simplicity, in this notebook we will only use 10 sections of the test set.

## Getting started
First, we make sure we are using Tensorflow version compatible with DeepImageJ (<= 1.13).
"""

# Commented out IPython magic to ensure Python compatibility.
# Use Tensorflow and Keras versions compatible with DeepImageJ
#%pip install tensorflow-gpu==2.5.0
#%pip install keras==2.2.4

"""Then, we load our Google Drive as a local folder so we can access the image files.

(Notice we expect you to have already this notebook under your `Colab Notebooks` in a folder called `U-Net-Super-resolution`. Inside that folder you should add the `train` and `test` image folders.)
"""

"""Now we should be able to read the list of **100 training images**."""

import os

# Path to the training images
train_path = 'C:/Users/HMXIF Remote/Harry/train_data_short/' # MAKE THIS INSTEAD OF 76 X 100 X 256 X 256 MAKE IT 7600 X 256 X 256

# Read the list of file names
train_filenames = [x for x in os.listdir( train_path ) if x.endswith(".tif")]

print( 'Images loaded: ' + str( len(train_filenames)) )
number_files = int(len(train_filenames))


"""Next, we read the 100 images into memory. This **may take some time (~1 minute)** the first time we execute the cell."""

from skimage.util import img_as_ubyte
from skimage import io
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow import keras
import numpy as np

# read training images
train_img = [ img_as_ubyte( io.imread( train_path + x ) ) for x in train_filenames ]

train_img_resized = []
for scan in train_img[:]:
    for cross_section in scan[:]:
        cross_section = cross_section#.astype(dtype='uint8')
        train_img_resized.append(cross_section)

train_img = train_img_resized
# display first image
plt.imshow( train_img[0], 'gray' )
plt.title( 'Full-size training image' )

"""## Preparing the training data
Now, we are going to create the training set by cropping the input images into **patches of 256 x 256 pixels**.
"""

# create patches of 256x256 pixels => split each image in 4x4 tiles

def create_patches( imgs, num_x_patches, num_y_patches ):
    ''' Create a list of images patches out of a list of images
    Args:
        imgs: list of input images
        num_x_patches: number of patches in the X axis
        num_y_patches: number of patches in the Y axis
        
    Returns:
        list of image patches
    '''
    original_size = imgs[0].shape
    patch_width = original_size[ 0 ] // num_x_patches
    patch_height = original_size[ 1 ] // num_y_patches
    
    patches = []
    for n in range( 0, len( imgs ) ):
        image = imgs[ n ]
        for i in range( 0, num_x_patches ):
            for j in range( 0, num_y_patches ):
                patches.append( image[ i * patch_width : (i+1) * patch_width,
                                      j * patch_height : (j+1) * patch_height ])#.astype(dtype='uint8') ) # All .astype comments can be removed for greater # of files (but ML is slower)
    return patches

# use method to create patches
train_patches = create_patches( train_img, 1, 1 )

# display one patch
plt.imshow( train_patches[0], 'gray' )
plt.title( 'Training patch at full resolution' )

# We will use these patches as "ground truth" for training

"""Since we do not have the equivalent images at low resolution, we **simulate them by using Gaussian blur** (see the work of [Fang *et al*. (2019)](https://www.biorxiv.org/content/10.1101/740548v3) for more details about this strategy)."""

from skimage import filters

# Create corresponding training patches synthetically by adding noise
# and downsampling the images (see https://www.biorxiv.org/content/10.1101/740548v3)

def classic_crappify(img):
    img = filters.gaussian(img, sigma=3) + 1e-6
    return img

# Add noise
crappified_patches = [classic_crappify(x) for x in train_patches]

# Display corresponding first patch at low resolution
plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.imshow( crappified_patches[0], 'gray' )
plt.title( 'Training patch at low resolution' )
# Side by side with its "ground truth"
plt.subplot(1, 2, 2)
plt.imshow( train_patches[0], 'gray' )
plt.title( 'Ground truth' )

# NOTE: images have now values between 0.0 and 1.0

"""## Network definition
Next, we define our U-Net-like network, with 3 resolution levels in the contracting path, a bottleneck, and 3 resolution levels in the expanding path.

As loss function, we use the mean squared error (MSE) between the expected and the predicted pixel values, and we also include the mean absolute error (MAE) as a control metric.
"""

# Input image size
patch_shape = train_patches[0].shape
train_width = 256
train_height = 256

# Create U-Net for super-resolution

from keras.models import Model
from keras.layers import Input, UpSampling2D
from keras.layers import Dropout
from keras.layers import Conv2D, Conv2DTranspose
from keras.layers import AveragePooling2D
from keras.layers import concatenate
from keras.optimizers import Adam
from tensorflow.python.keras.layers import Input

inputs = keras.layers.Input(shape=(train_width, train_height, 1))
#x_1 = keras.layers.Input(shape=(x_size, y_size))

c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)
c1 = Dropout(0.1) (c1)
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)
p1 = AveragePooling2D((2, 2)) (c1)

c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)
c2 = Dropout(0.1) (c2)
c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)
p2 = AveragePooling2D((2, 2)) (c2)

c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)
c3 = Dropout(0.2) (c3)
c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)
p3 = AveragePooling2D((2, 2)) (c3)

c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)
c4 = Dropout(0.2) (c4)
c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)

u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c4)
u5 = concatenate([u5, c3])
c5 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u5)
c5 = Dropout(0.2) (c5)
c5 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)

u6 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c5)
u6 = concatenate([u6, c2])
c6 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)
c6 = Dropout(0.1) (c6)
c6 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)

u7 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c6)
u7 = concatenate([u7, c1], axis=3)
c7 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)
c7 = Dropout(0.1) (c7)
c7 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)

outputs = Conv2D(1, (1, 1), activation='sigmoid') (c7)

model = Model(inputs=[inputs], outputs=[outputs])
# compile the model with RMSProp as optimizer, MSE as loss function and MAE as metric
model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])
model.summary()

"""## Training the network
To follow Tensorflow standards, the input and output of the network have to be reshaped 256 x 256 x 1. Notice both input and ground truth images have their intensities scaled between 0.0 and 1.0.

Important training information:
*   `Validation split`: percentage of training samples used for validation. Set to a random 10%.
*   `Epochs`: which defines the maximum number of epochs the model will be trained. Initially set to 20.
*   `Patience`: number of epochs that produced the monitored quantity (validation MSE) with no improvement after which training will be stopped. Initially set to 5.
*   `Batch size`:  the number of training examples in one forward/backward pass. Initially set to 6.
"""

# Train the network
from keras.callbacks import EarlyStopping

numEpochs = 10 # i.e. number of times the training data set has had the chance to update the model parameters
earlystopper = EarlyStopping(patience=5, verbose=1, restore_best_weights=True)

# training input
input_shape = ( train_width, train_height, 1 ) # 256x256x1
X_train = [np.reshape(x, input_shape ) for x in crappified_patches] # Blurred images
X_train = np.asarray(X_train)

# training ground truth
output_shape = ( train_width, train_height, 1 ) # 256x256x1
Y_train = [x/255 for x in train_patches[:]] # normalize between 0 and 1
Y_train = [np.reshape( x, output_shape ) for x in Y_train] # Clear images
Y_train = np.asarray(Y_train)

#print("X train array: ", X_train)
#print("Y train array: ", Y_train)

"""
# training input
file = 0
X_train = np.array([])
while (file<number_files):
    for scan in crappified_patches:        
        input_shape = ( train_width, train_height, 1 ) # 256x256x1
        X_train_scan = [np.reshape(x, input_shape ) for x in scan]
        X_train_scan = np.asarray(X_train_scan)
        X_train = np.append(X_train, X_train_scan)
        file += 1

# training ground truth

file = 0
Y_train = np.array([])
while (file<number_files):
    for scan in train_patches:        
        input_shape = ( train_width, train_height, 1 ) # 256x256x1
        Y_train_scan = [np.reshape(x, input_shape ) for x in scan]
        Y_train_scan = np.asarray(Y_train_scan)
        Y_train = np.append(Y_train, Y_train_scan)
        file += 1

X_train = np.reshape(X_train, [number_files, 1, 1, int(len(X_train)/number_files)])
Y_train = np.reshape(Y_train, [number_files, 1, 1, int(len(Y_train)/number_files)])
"""
history = model.fit(X_train, Y_train, validation_split=0.1, batch_size = 6,
                    epochs=numEpochs, callbacks=[earlystopper])

"""We can now plot the loss and MAE curves for the training and validation sets."""

import matplotlib.pyplot as plt

plt.figure(figsize=(14,5))

# summarize history for loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

# summarize history for MAE
plt.subplot(1, 2, 2)
plt.plot(history.history['mean_absolute_error'])
plt.plot(history.history['val_mean_absolute_error'])
plt.title('model MAE')
plt.ylabel('MAE')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""## Check performance in the test set
Finally we can load some test images for testing.
"""

# Now we load some unseen images for testing
test_path = 'C:/Users/HMXIF Remote/Harry/test_data/'

test_filenames = [x for x in os.listdir( test_path ) if x.endswith(".tif")]



# train_img = [ img_as_ubyte( io.imread( train_path + x ) ) for x in train_filenames ]

# train_img_resized = []
# for scan in train_img[:]:
#     for cross_section in scan[:]:
#         train_img_resized.append(cross_section)

# train_img = train_img_resized

print( 'Available test images: ' + str( len(test_filenames)) )

# Read test images
test_img = [ img_as_ubyte( io.imread( test_path + x ) ) for x in test_filenames ]
test_img_resized = []

for scan in test_img[:]:
    for cross_section in scan[:]:
        test_img_resized.append(cross_section)
        
test_img = test_img_resized

# Create patches the same way as before
test_patches = create_patches( test_img, 1, 1 )

# Add noise
crappified_test_patches = [classic_crappify(x) for x in test_patches]

# Display corresponding first patch at low resolution
plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.imshow( crappified_test_patches[0], 'gray' )
plt.title( 'Test patch at low resolution' )
# Side by side with its "ground truth"
plt.subplot(1, 2, 2)
plt.imshow( test_patches[0], 'gray' )
plt.title( 'Ground truth' )

"""We can evaluate the network performance in test using both the MSE and MAE metrics."""

patch_shape = test_patches[0].shape
test_width = 256
test_height = 256
input_shape = ( test_width, test_height, 1 )

# Evaluate trained network on test images
X_test = [np.reshape(x, input_shape ) for x in crappified_test_patches]
X_test = np.asarray(X_test)
output_shape = ( test_width, test_height, 1 )

Y_test = [x/255 for x in test_patches] # normalize between 0 and 1
Y_test = [np.reshape( x, output_shape ) for x in Y_test]
Y_test = np.asarray(Y_test)

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(X_test, Y_test , batch_size=16)
print('test loss, test MAE:', results)

"""And also display some patches for qualitative evaluation."""

print('\n# Generate predictions for 3 samples')
predictions = model.predict(X_test[:,:,:,:])
print('predictions shape:', predictions.shape)

# Display corresponding first 3 patches
plt.figure(figsize=(15,15))
plt.subplot(3, 3, 1)
plt.imshow( crappified_test_patches[0], 'gray' )
plt.title( 'Test patch at low resolution' )
# Side by side with its "ground truth"
plt.subplot(3, 3, 2)
plt.imshow( test_patches[0], 'gray' )
plt.title( 'Ground truth' )
# and its prediction
plt.subplot(3, 3, 3)
plt.imshow( predictions[0,:,:,0], 'gray' )
plt.title( 'Prediction' )

plt.subplot(3, 3, 4)
plt.imshow( crappified_test_patches[1], 'gray' )
plt.title( 'Test patch at low resolution' )
# Side by side with its "ground truth"
plt.subplot(3, 3, 5)
plt.imshow( test_patches[1], 'gray' )
plt.title( 'Ground truth' )
# and its prediction
plt.subplot(3, 3, 6)
plt.imshow( predictions[1,:,:,0], 'gray' )
plt.title( 'Prediction' )

plt.subplot(3, 3, 7)
plt.imshow( crappified_test_patches[2], 'gray' )
plt.title( 'Test patch at low resolution' )
# Side by side with its "ground truth"
plt.subplot(3, 3, 8)
plt.imshow( test_patches[2], 'gray' )
plt.title( 'Ground truth' )
# and its prediction
plt.subplot(3, 3, 9)
plt.imshow( predictions[2,:,:,0], 'gray' )
plt.title( 'Prediction' )

deviations = np.array([])
for index, cross_section in enumerate(test_patches):
    avg_standard_deviation_error = np.std(cross_section - predictions[index, :, :, 0])/(256**2)
    deviations = np.append(deviations, avg_standard_deviation_error)

print(np.mean(deviations))


"""## Save model to import it later in DeepImageJ
Now, we will see how to saved the train model into a file so we can later reuse it in the DeepImageJ plugin.
"""

# !rm -rf save_model

# Save entire model to the Tensorflow format SavedModel

#import tensorflow as tf
import keras
from keras import backend as K

OUTPUT_DIR = "C:/Users/HMXIF Remote/Harry/ML_results_and_models/saved_model"

model.save(OUTPUT_DIR)

# TO RE-OPEN THE MODEL AGAIN

new_model = tf.keras.models.load_model(OUTPUT_DIR)


# builder = tf.saved_model.builder.SavedModelBuilder(OUTPUT_DIR)

# signature = tf.saved_model.signature_def_utils.predict_signature_def(
#              inputs  = {'input':  model.input},
#              outputs = {'output': model.output})

# signature_def_map = { tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature }

# builder.add_meta_graph_and_variables(K.get_session(), [tf.saved_model.tag_constants.SERVING],
#                                               signature_def_map=signature_def_map)
# builder.save()

# # Check if the folder has been properly created
#!ls -l

# # Zip folder with the save model
#from google.colab import files

# !zip saved_model -r saved_model/

# # Check if the file is there
# !ls -l

# # And download!
# files.download("saved_model.zip")